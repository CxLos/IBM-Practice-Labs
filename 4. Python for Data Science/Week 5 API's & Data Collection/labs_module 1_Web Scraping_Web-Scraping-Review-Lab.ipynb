{"cells":[{"cell_type":"markdown","id":"a1477b8f-a8d7-47b1-8757-56444dad8a91","metadata":{},"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0101EN-SkillsNetwork/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n","</center>\n"]},{"cell_type":"markdown","id":"76a04e25-ae4f-4d78-b342-2a0197511ea6","metadata":{},"source":["# **Web Scraping Lab**\n"]},{"cell_type":"markdown","id":"93c3de1d-e860-4592-9dee-59a14167989d","metadata":{},"source":["Estimated time needed: **40** minutes\n"]},{"cell_type":"markdown","id":"5dae311e-39e9-4e67-9e9a-054c21910f7e","metadata":{},"source":["## Objectives\n"]},{"cell_type":"markdown","id":"31d9f62a-0252-4190-82ff-514d5d48243b","metadata":{},"source":["After completing this lab you will be:\n","\n","*   Familiar with the basics of the `BeautifulSoup` Python library\n","*   Be able to scrape webpages for data and filter the data\n"]},{"cell_type":"markdown","id":"e95300a2-3276-4d2c-a74f-530fe615b0d8","metadata":{},"source":["<h2>Table of Contents</h2>\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","    <ul>\n","        <li>\n","            <a href=\"#Beautiful-Soup-Object\">Beautiful Soup Object</a>\n","            <ul>\n","                <li><a href=\"#Tags\">Tags</a></li>\n","                <li><a href=\"#Children,-Parents,-and-Siblings\">Children, Parents, and Siblings</a></li>\n","                <li><a href=\"#HTML-Attributes\">HTML Attributes</a></li>\n","                <li><a href=\"#Navigable-String\">Navigable String</a></li>\n","            </ul>\n","        </li>\n","     </ul>\n","    <ul>\n","        <li>\n","            <a href=\"#Filter\">Filter</a>\n","            <ul>\n","                <li><a href=\"#find_All\">find_All</a></li>\n","                <li><a href=\"#find\">find</a></li>\n","            </ul>\n","        </li>\n","     </ul>\n","     <ul>\n","        <li>\n","            <a href=\"#Downloading-And-Scraping-The-Contents-Of-A-Web-Page\">Downloading And Scraping The Contents Of A Web Page</a></li>\n","         <li> <a href=\"#Scraping-tables-from-a-Web-page-using-Pandas\">Scraping tables from a Web page using Pandas</a></li>\n","    </ul>\n","\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"6c827db0-2d8f-4957-83bb-69f9935b8f18","metadata":{},"source":["For this lab, we are going to be using Python and several Python libraries. Some of these libraries might be installed in your lab environment or in SN Labs. Others may need to be installed by you. The cells below will install these libraries when executed.\n"]},{"cell_type":"code","execution_count":1,"id":"0428d96e-71b6-45df-8bbd-842728086594","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: html5lib in c:\\users\\cxlos\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1)\n","Requirement already satisfied: six>=1.9 in c:\\users\\cxlos\\appdata\\roaming\\python\\python312\\site-packages (from html5lib) (1.16.0)\n","Requirement already satisfied: webencodings in c:\\users\\cxlos\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from html5lib) (0.5.1)\n"]}],"source":["!pip install html5lib"]},{"cell_type":"markdown","id":"1f24c9f9-86d2-428c-ab81-bf54f083f9ca","metadata":{},"source":["**Note:- After running the above code cell, restart the kernel and don't run the above code cell after restarting the kernel.**\n"]},{"cell_type":"code","execution_count":2,"id":"c3783715-8016-402d-bd37-ae95ea7fa5a3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting bs4\n","  Using cached bs4-0.0.1-py3-none-any.whl\n","Collecting beautifulsoup4 (from bs4)\n","  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n","Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n","  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n","Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n","Installing collected packages: soupsieve, beautifulsoup4, bs4\n","Successfully installed beautifulsoup4-4.12.2 bs4-0.0.1 soupsieve-2.5\n"]}],"source":["!pip install bs4\n","#!pip install requests"]},{"cell_type":"markdown","id":"d4d5da7f-e04d-49da-a6a2-1d2406e2426c","metadata":{},"source":["Import the required modules and functions\n"]},{"cell_type":"code","execution_count":3,"id":"c91808fa-4e9a-43d8-8786-2b55e504fe83","metadata":{},"outputs":[],"source":["from bs4 import BeautifulSoup # this module helps in web scrapping.\n","import requests  # this module helps us to download a web page"]},{"cell_type":"markdown","id":"cd501e03-9401-4e9b-955e-ab77abadb5b3","metadata":{},"source":["## Beautiful Soup Object\n"]},{"cell_type":"markdown","id":"206ee994-c30d-489b-b23f-6594696d69c4","metadata":{},"source":["Beautiful Soup is a Python library for pulling data out of HTML and XML files, we will focus on HTML files. This is accomplished by representing the HTML as a set of objects with methods used to parse the HTML.  We can navigate the HTML as a tree, and/or filter out what we are looking for.\n","\n","Consider the following HTML:\n"]},{"cell_type":"code","execution_count":4,"id":"7e810b0d-614d-433e-891e-957a69b69c78","metadata":{},"outputs":[{"data":{"text/html":["<!DOCTYPE html>\n","<html>\n","<head>\n","<title>Page Title</title>\n","</head>\n","<body>\n","<h3><b id='boldest'>Lebron James</b></h3>\n","<p> Salary: $ 92,000,000 </p>\n","<h3> Stephen Curry</h3>\n","<p> Salary: $85,000, 000 </p>\n","<h3> Kevin Durant </h3>\n","<p> Salary: $73,200, 000</p>\n","</body>\n","</html>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["%%html\n","<!DOCTYPE html>\n","<html>\n","<head>\n","<title>Page Title</title>\n","</head>\n","<body>\n","<h3><b id='boldest'>Lebron James</b></h3>\n","<p> Salary: $ 92,000,000 </p>\n","<h3> Stephen Curry</h3>\n","<p> Salary: $85,000, 000 </p>\n","<h3> Kevin Durant </h3>\n","<p> Salary: $73,200, 000</p>\n","</body>\n","</html>"]},{"cell_type":"markdown","id":"1da3a0d5-0905-42db-842b-c69763deb13c","metadata":{},"source":["We can store it as a string in the variable HTML:\n"]},{"cell_type":"code","execution_count":5,"id":"e92f7643-15ea-4f8a-8e02-bb54a81041ad","metadata":{},"outputs":[],"source":["html=\"<!DOCTYPE html><html><head><title>Page Title</title></head><body><h3><b id='boldest'>Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body></html>\""]},{"cell_type":"markdown","id":"de177f38-15f1-431c-8d11-36113ad6690f","metadata":{},"source":["To parse a document, pass it into the <code>BeautifulSoup</code> constructor. The <code>BeautifulSoup</code> object represents the document as a nested data structure:\n"]},{"cell_type":"code","execution_count":6,"id":"aa18d377-2b5c-4452-bf84-b61e653f6e8b","metadata":{},"outputs":[],"source":["soup = BeautifulSoup(html, 'html5lib')"]},{"cell_type":"markdown","id":"8dc8d60e-1f03-4260-906c-2dd0484e357d","metadata":{},"source":["First, the document is converted to Unicode (similar to ASCII) and HTML entities are converted to Unicode characters. Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. The <code>BeautifulSoup</code> object can create other types of objects. In this lab, we will cover <code>BeautifulSoup</code> and <code>Tag</code> objects, that for the purposes of this lab are identical. Finally, we will look at <code>NavigableString</code> objects.\n"]},{"cell_type":"markdown","id":"c16679d4-fa8c-42a3-a490-39c5477f6886","metadata":{},"source":["We can use the method <code>prettify()</code> to display the HTML in the nested structure:\n"]},{"cell_type":"code","execution_count":7,"id":"a08cff02-80d3-4107-9454-b21770756ec4","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<!DOCTYPE html>\n","<html>\n"," <head>\n","  <title>\n","   Page Title\n","  </title>\n"," </head>\n"," <body>\n","  <h3>\n","   <b id=\"boldest\">\n","    Lebron James\n","   </b>\n","  </h3>\n","  <p>\n","   Salary: $ 92,000,000\n","  </p>\n","  <h3>\n","   Stephen Curry\n","  </h3>\n","  <p>\n","   Salary: $85,000, 000\n","  </p>\n","  <h3>\n","   Kevin Durant\n","  </h3>\n","  <p>\n","   Salary: $73,200, 000\n","  </p>\n"," </body>\n","</html>\n","\n"]}],"source":["print(soup.prettify())"]},{"cell_type":"markdown","id":"78259c87-f479-4aed-a35e-6e442a1c7377","metadata":{},"source":["## Tags\n"]},{"cell_type":"markdown","id":"c65fef1c-2345-4b78-a7b8-33bcc5feb312","metadata":{},"source":["Let's say we want the  title of the page and the name of the top paid player. We can use the <code>Tag</code>. The <code>Tag</code> object corresponds to an HTML tag in the original document, for example, the tag title.\n"]},{"cell_type":"code","execution_count":8,"id":"22043cc8-2059-4512-bb30-e83f0a2ad3d0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tag object: <title>Page Title</title>\n"]}],"source":["tag_object=soup.title\n","print(\"tag object:\",tag_object)"]},{"cell_type":"markdown","id":"bef2e51e-a5a3-452b-8c1e-bb500ffe1d65","metadata":{},"source":["we can see the tag type <code>bs4.element.Tag</code>\n"]},{"cell_type":"code","execution_count":9,"id":"30cedc94-e258-4da8-82e1-08de13e5104b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tag object type: <class 'bs4.element.Tag'>\n"]}],"source":["print(\"tag object type:\",type(tag_object))"]},{"cell_type":"markdown","id":"f125ff93-4d10-47a1-b853-cfcf593f8ff6","metadata":{},"source":["If there is more than one <code>Tag</code> with the same name, the first element with that <code>Tag</code> name is called. This corresponds to the most paid player:\n"]},{"cell_type":"code","execution_count":10,"id":"bbbf01e9-7518-49ca-bfcd-9247f76be188","metadata":{},"outputs":[{"data":{"text/plain":["<h3><b id=\"boldest\">Lebron James</b></h3>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["tag_object=soup.h3\n","tag_object"]},{"cell_type":"markdown","id":"c6834365-e3aa-4d59-967a-919b3eab39b8","metadata":{},"source":["Enclosed in the bold attribute <code>b</code>, it helps to use the tree representation. We can navigate down the tree using the child attribute to get the name.\n"]},{"cell_type":"markdown","id":"a2f99c47-22f2-4d23-b44a-084797d0de61","metadata":{},"source":["### Children, Parents, and Siblings\n"]},{"cell_type":"markdown","id":"95aa8115-e51c-4a70-8912-de6e45adc50c","metadata":{},"source":["As stated above, the <code>Tag</code> object is a tree of objects. We can access the child of the tag or navigate down the branch as follows:\n"]},{"cell_type":"code","execution_count":12,"id":"437cc21f-34b8-438a-b1a2-d321ddc90ddc","metadata":{},"outputs":[{"data":{"text/plain":["<b id=\"boldest\">Lebron James</b>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["tag_child =tag_object.b\n","tag_child"]},{"cell_type":"markdown","id":"cb3039ad-7e29-4200-9c7f-a4bb0d491f3a","metadata":{},"source":["You can access the parent with the <code> parent</code>.\n"]},{"cell_type":"code","execution_count":13,"id":"3577e9f0-1d58-4735-902e-4f4ec13732d0","metadata":{},"outputs":[{"data":{"text/plain":["<h3><b id=\"boldest\">Lebron James</b></h3>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["parent_tag=tag_child.parent\n","parent_tag"]},{"cell_type":"markdown","id":"ad242a4a-06a5-4a8d-961c-099e8a2b8ae6","metadata":{},"source":["this is identical to:\n"]},{"cell_type":"code","execution_count":14,"id":"ee267842-01ec-448d-9f38-4e1426001575","metadata":{},"outputs":[{"data":{"text/plain":["<h3><b id=\"boldest\">Lebron James</b></h3>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["tag_object"]},{"cell_type":"markdown","id":"0a5828cd-f962-48ed-8e61-a49f05f358f3","metadata":{},"source":["<code>tag_object</code> parent is the <code>body</code> element.\n"]},{"cell_type":"code","execution_count":15,"id":"00755a73-02ae-483f-90ad-88d4bb82d779","metadata":{},"outputs":[{"data":{"text/plain":["<body><h3><b id=\"boldest\">Lebron James</b></h3><p> Salary: $ 92,000,000 </p><h3> Stephen Curry</h3><p> Salary: $85,000, 000 </p><h3> Kevin Durant </h3><p> Salary: $73,200, 000</p></body>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["tag_object.parent"]},{"cell_type":"markdown","id":"783efde9-e42b-4262-b996-1dbb128a0533","metadata":{},"source":["<code>tag_object</code> sibling is the <code>paragraph</code> element.\n"]},{"cell_type":"code","execution_count":16,"id":"fc246920-2197-41cc-ae6e-80ee1d19f336","metadata":{},"outputs":[{"data":{"text/plain":["<p> Salary: $ 92,000,000 </p>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["sibling_1=tag_object.next_sibling\n","sibling_1"]},{"cell_type":"markdown","id":"ad43649b-6c29-4eae-a9fd-c103bc1dba2a","metadata":{},"source":["`sibling_2` is the `header` element, which is also a sibling of both `sibling_1` and `tag_object`\n"]},{"cell_type":"code","execution_count":17,"id":"68b4f9fa-3322-4296-9b77-67656346f72c","metadata":{},"outputs":[{"data":{"text/plain":["<h3> Stephen Curry</h3>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["sibling_2=sibling_1.next_sibling\n","sibling_2"]},{"cell_type":"markdown","id":"2385f2cc-869e-4ee6-8de7-5aef7bd09198","metadata":{},"source":["<h3 id=\"first_question\">Exercise: <code>next_sibling</code></h3>\n"]},{"cell_type":"markdown","id":"c139e030-51cd-4583-bb12-f3bc515b554a","metadata":{},"source":["Use the object <code>sibling\\_2</code> and the method <code>next_sibling</code> to find the salary of Stephen Curry:\n"]},{"cell_type":"code","execution_count":18,"id":"59854658-f103-474b-93af-f1ad42ce3a61","metadata":{},"outputs":[{"data":{"text/plain":["<h3> Stephen Curry</h3>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["sibling_3=sibling_1.next_sibling\n","sibling_3"]},{"cell_type":"markdown","id":"6a573f0e-4c94-4687-b4ad-f935b172eac9","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```\n","sibling_2.next_sibling\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"c42a04c3-3d8a-48d1-b182-66450f8701ff","metadata":{},"source":["### HTML Attributes\n"]},{"cell_type":"markdown","id":"95545613-a8cf-4799-8384-3bd232ab59cd","metadata":{},"source":["If the tag has attributes, the tag <code>id=\"boldest\"</code> has an attribute <code>id</code> whose value is <code>boldest</code>. You can access a tag's attributes by treating the tag like a dictionary:\n"]},{"cell_type":"code","execution_count":19,"id":"2bb697aa-ba5b-4220-ac9c-d2ea22aa8a01","metadata":{},"outputs":[{"data":{"text/plain":["'boldest'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["tag_child['id']"]},{"cell_type":"markdown","id":"2afb4d01-5040-4464-913a-b220b666a3c7","metadata":{},"source":["You can access that dictionary directly as <code>attrs</code>:\n"]},{"cell_type":"code","execution_count":20,"id":"baf7f1b1-ecc3-45bf-98cb-cacc2166d542","metadata":{},"outputs":[{"data":{"text/plain":["{'id': 'boldest'}"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["tag_child.attrs"]},{"cell_type":"markdown","id":"081b9bb6-e6ab-43b0-84c0-9e2542581b08","metadata":{},"source":["You can also work with Multi-valued attributes. Check out <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01\">\\[1]</a> for more.\n"]},{"cell_type":"markdown","id":"5bcb61d1-c63f-4f47-b975-f920ccd12e65","metadata":{},"source":["We can also obtain the content of the attribute of the <code>tag</code> using the Python <code>get()</code> method.\n"]},{"cell_type":"code","execution_count":21,"id":"993017b9-e3a3-4eae-8d0c-02ce3f5787d4","metadata":{},"outputs":[{"data":{"text/plain":["'boldest'"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["tag_child.get('id')"]},{"cell_type":"markdown","id":"7166293e-6803-48a8-8163-a5892443b1ae","metadata":{},"source":["### Navigable String\n"]},{"cell_type":"markdown","id":"7e630937-96f5-4359-b668-8174232993ad","metadata":{},"source":["A string corresponds to a bit of text or content within a tag. Beautiful Soup uses the <code>NavigableString</code> class to contain this text. In our HTML we can obtain the name of the first player by extracting the string of the <code>Tag</code> object <code>tag_child</code> as follows:\n"]},{"cell_type":"code","execution_count":22,"id":"74a820e3-8f27-4e0f-891c-da37e63884c3","metadata":{},"outputs":[{"data":{"text/plain":["'Lebron James'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["tag_string=tag_child.string\n","tag_string"]},{"cell_type":"markdown","id":"d573ea8c-6937-42ca-ac7f-c70505d200a7","metadata":{},"source":["we can verify the type is Navigable String\n"]},{"cell_type":"code","execution_count":23,"id":"ca55c077-6c1c-45b8-8c9a-f81aaa95d3b1","metadata":{},"outputs":[{"data":{"text/plain":["bs4.element.NavigableString"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["type(tag_string)"]},{"cell_type":"markdown","id":"03020fee-6a50-4f78-a460-934414330961","metadata":{},"source":["A NavigableString is similar to a Python string or Unicode string. To be more precise, the main difference is that it also supports some <code>BeautifulSoup</code> features. We can convert it to string object in Python:\n"]},{"cell_type":"code","execution_count":24,"id":"cb523ce9-2522-48a5-890c-5d92f1f78240","metadata":{},"outputs":[{"data":{"text/plain":["'Lebron James'"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["unicode_string = str(tag_string)\n","unicode_string"]},{"cell_type":"markdown","id":"122f5c28-d2ba-49b0-94ea-d90610c44177","metadata":{},"source":["## Filter\n"]},{"cell_type":"markdown","id":"d42cf065-d0ad-4124-8b94-a8fae600b0d4","metadata":{},"source":["Filters allow you to find complex patterns, the simplest filter is a string. In this section we will pass a string to a different filter method and Beautiful Soup will perform a match against that exact string. Consider the following HTML of rocket launches:\n"]},{"cell_type":"code","execution_count":25,"id":"04bee9fb-1446-481b-9661-7440554d4074","metadata":{},"outputs":[{"data":{"text/html":["<table>\n","  <tr>\n","    <td id='flight' >Flight No</td>\n","    <td>Launch site</td> \n","    <td>Payload mass</td>\n","   </tr>\n","  <tr> \n","    <td>1</td>\n","    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n","    <td>300 kg</td>\n","  </tr>\n","  <tr>\n","    <td>2</td>\n","    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n","    <td>94 kg</td>\n","  </tr>\n","  <tr>\n","    <td>3</td>\n","    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td>\n","    <td>80 kg</td>\n","  </tr>\n","</table>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["%%html\n","<table>\n","  <tr>\n","    <td id='flight' >Flight No</td>\n","    <td>Launch site</td> \n","    <td>Payload mass</td>\n","   </tr>\n","  <tr> \n","    <td>1</td>\n","    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>\n","    <td>300 kg</td>\n","  </tr>\n","  <tr>\n","    <td>2</td>\n","    <td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>\n","    <td>94 kg</td>\n","  </tr>\n","  <tr>\n","    <td>3</td>\n","    <td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td>\n","    <td>80 kg</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","id":"3c3a7012-5433-4ce1-a7d0-f52abbb50482","metadata":{},"source":["We can store it as a string in the variable <code>table</code>:\n"]},{"cell_type":"code","execution_count":26,"id":"b7dac682-3e5c-458a-b0fe-fd8abe6b64b5","metadata":{},"outputs":[],"source":["table=\"<table><tr><td id='flight'>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a></td><td>300 kg</td></tr><tr><td>2</td><td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida<a> </td><td>80 kg</td></tr></table>\""]},{"cell_type":"code","execution_count":27,"id":"cf406046-5be5-4025-bf87-436eb366e21b","metadata":{},"outputs":[],"source":["table_bs = BeautifulSoup(table, 'html5lib')"]},{"cell_type":"markdown","id":"e7176649-ae82-444c-a4b7-f87d6731fb3c","metadata":{},"source":["## find_All\n"]},{"cell_type":"markdown","id":"763f0eb5-4abd-48b1-8f7d-0362da7cfa27","metadata":{},"source":["The <code>find_all()</code> method looks through a tag's descendants and retrieves all descendants that match your filters.\n","\n","<p>\n","The Method signature for <code>find_all(name, attrs, recursive, string, limit, **kwargs)<c/ode>\n","</p>\n"]},{"cell_type":"markdown","id":"2b1b490e-58d0-4e7e-a345-83525ba130c7","metadata":{},"source":["### Name\n"]},{"cell_type":"markdown","id":"4704ef45-47f5-48ef-a2dc-a3c711e65423","metadata":{},"source":["When we set the <code>name</code> parameter to a tag name, the method will extract all the tags with that name and its children.\n"]},{"cell_type":"code","execution_count":28,"id":"e2793cd7-9833-46a0-a3d0-7fcb3324b489","metadata":{},"outputs":[{"data":{"text/plain":["[<tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>,\n"," <tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td><td>300 kg</td></tr>,\n"," <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n"," <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td><td>80 kg</td></tr>]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["table_rows=table_bs.find_all('tr')\n","table_rows"]},{"cell_type":"markdown","id":"3040b3c5-f69f-4a17-8f57-78248d38fd47","metadata":{},"source":["The result is a Python iterable just like a list, each element is a <code>tag</code> object:\n"]},{"cell_type":"code","execution_count":29,"id":"80ef402c-323b-4334-8052-90cfc7c3e2df","metadata":{},"outputs":[{"data":{"text/plain":["<tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["first_row =table_rows[0]\n","first_row"]},{"cell_type":"markdown","id":"85f7d547-ca41-4690-bac1-3fc12caed447","metadata":{},"source":["The type is <code>tag</code>\n"]},{"cell_type":"code","execution_count":30,"id":"1092f8e2-be14-49da-8e91-9d4950a0adeb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'bs4.element.Tag'>\n"]}],"source":["print(type(first_row))"]},{"cell_type":"markdown","id":"d9dc1fd9-8ca1-405a-8ace-b1d78129cc0c","metadata":{},"source":["we can obtain the child\n"]},{"cell_type":"code","execution_count":31,"id":"cdb0caab-eb13-4bf3-82fa-83518e8671f1","metadata":{},"outputs":[{"data":{"text/plain":["<td id=\"flight\">Flight No</td>"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["first_row.td"]},{"cell_type":"markdown","id":"2d229567-721d-4fff-8bfc-1fdd69211f38","metadata":{},"source":["If we iterate through the list, each element corresponds to a row in the table:\n"]},{"cell_type":"code","execution_count":32,"id":"f00cc0d6-e600-4e01-b1c5-795ee1b67804","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["row 0 is <tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>\n","row 1 is <tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td><td>300 kg</td></tr>\n","row 2 is <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>\n","row 3 is <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td><td>80 kg</td></tr>\n"]}],"source":["for i,row in enumerate(table_rows):\n","    print(\"row\",i,\"is\",row)\n","    "]},{"cell_type":"markdown","id":"5ebf6cb5-652f-47ed-ac9e-6a52f0270bbc","metadata":{},"source":["As <code>row</code> is a <code>cell</code> object, we can apply the method <code>find_all</code> to it and extract table cells in the object <code>cells</code> using the tag <code>td</code>, this is all the children with the name <code>td</code>. The result is a list, each element corresponds to a cell and is a <code>Tag</code> object, we can iterate through this list as well. We can extract the content using the <code>string</code> attribute.\n"]},{"cell_type":"code","execution_count":33,"id":"32878dff-acd7-4e85-a3ee-0450f691df0f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["row 0\n","colunm 0 cell <td id=\"flight\">Flight No</td>\n","colunm 1 cell <td>Launch site</td>\n","colunm 2 cell <td>Payload mass</td>\n","row 1\n","colunm 0 cell <td>1</td>\n","colunm 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td>\n","colunm 2 cell <td>300 kg</td>\n","row 2\n","colunm 0 cell <td>2</td>\n","colunm 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>\n","colunm 2 cell <td>94 kg</td>\n","row 3\n","colunm 0 cell <td>3</td>\n","colunm 1 cell <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td>\n","colunm 2 cell <td>80 kg</td>\n"]}],"source":["for i,row in enumerate(table_rows):\n","    print(\"row\",i)\n","    cells=row.find_all('td')\n","    for j,cell in enumerate(cells):\n","        print('colunm',j,\"cell\",cell)"]},{"cell_type":"markdown","id":"22e56f75-3cdf-4df7-a617-b65df9c9a380","metadata":{},"source":["If we use a list we can match against any item in that list.\n"]},{"cell_type":"code","execution_count":34,"id":"d77c24bd-d9dd-4c48-ada7-995c7888765c","metadata":{},"outputs":[{"data":{"text/plain":["[<tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>,\n"," <td id=\"flight\">Flight No</td>,\n"," <td>Launch site</td>,\n"," <td>Payload mass</td>,\n"," <tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td><td>300 kg</td></tr>,\n"," <td>1</td>,\n"," <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td>,\n"," <td>300 kg</td>,\n"," <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n"," <td>2</td>,\n"," <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>,\n"," <td>94 kg</td>,\n"," <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td><td>80 kg</td></tr>,\n"," <td>3</td>,\n"," <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td>,\n"," <td>80 kg</td>]"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["list_input=table_bs .find_all(name=[\"tr\", \"td\"])\n","list_input"]},{"cell_type":"markdown","id":"3d19ed0d-e1e5-495b-97d4-39873d91577f","metadata":{},"source":["### Attributes\n"]},{"cell_type":"markdown","id":"f1a2b1d2-daac-48a3-b27f-710ac1db020d","metadata":{},"source":["If the argument is not recognized it will be turned into a filter on the tag's attributes. For example with the <code>id</code> argument, Beautiful Soup will filter against each tag's <code>id</code> attribute. For example, the first <code>td</code> elements have a value of <code>id</code> of <code>flight</code>, therefore we can filter based on that <code>id</code> value.\n"]},{"cell_type":"code","execution_count":35,"id":"432269b4-4247-4a57-be16-d5ffb5a1df84","metadata":{},"outputs":[{"data":{"text/plain":["[<td id=\"flight\">Flight No</td>]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["table_bs.find_all(id=\"flight\")"]},{"cell_type":"markdown","id":"f085cd43-7b3b-40f0-b3db-21a852c6fe75","metadata":{},"source":["We can find all the elements that have links to the Florida Wikipedia page:\n"]},{"cell_type":"code","execution_count":36,"id":"570b298c-b955-4ffa-9f62-f4325b59a69b","metadata":{},"outputs":[{"data":{"text/plain":["[<a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>,\n"," <a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>]"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["list_input=table_bs.find_all(href=\"https://en.wikipedia.org/wiki/Florida\")\n","list_input"]},{"cell_type":"markdown","id":"c0a1cdd5-8e06-420f-9ecb-a966e164d2ad","metadata":{},"source":["If we set the <code>href</code> attribute to True, regardless of what the value is, the code finds all tags with <code>href</code> value:\n"]},{"cell_type":"code","execution_count":37,"id":"7947bc41-1d2d-46a3-bf74-d15289a167b6","metadata":{},"outputs":[{"data":{"text/plain":["[<a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>,\n"," <a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a>,\n"," <a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a>]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["table_bs.find_all(href=True)"]},{"cell_type":"markdown","id":"86fa22e9-f94b-400d-b37c-d04b9f9985ed","metadata":{},"source":["There are other methods for dealing with attributes and other related methods. Check out the following <a href='https://www.crummy.com/software/BeautifulSoup/bs4/doc/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01#css-selectors'>link</a>\n"]},{"cell_type":"markdown","id":"0e2a5300-23ec-42e1-b09e-525cd48415c9","metadata":{},"source":["<h3 id=\"exer_type\">Exercise: <code>find_all</code></h3>\n"]},{"cell_type":"markdown","id":"b7889a6d-4809-470b-b3c5-9666988efe5e","metadata":{},"source":["Using the logic above, find all the elements without <code>href</code> value\n"]},{"cell_type":"code","execution_count":38,"id":"1b9d3f37-6aac-4126-bca5-cff78e5675f4","metadata":{},"outputs":[{"data":{"text/plain":["[<html><head></head><body><table><tbody><tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td><td>300 kg</td></tr><tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td><td>80 kg</td></tr></tbody></table></body></html>,\n"," <head></head>,\n"," <body><table><tbody><tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td><td>300 kg</td></tr><tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td><td>80 kg</td></tr></tbody></table></body>,\n"," <table><tbody><tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td><td>300 kg</td></tr><tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td><td>80 kg</td></tr></tbody></table>,\n"," <tbody><tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td><td>300 kg</td></tr><tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td><td>80 kg</td></tr></tbody>,\n"," <tr><td id=\"flight\">Flight No</td><td>Launch site</td> <td>Payload mass</td></tr>,\n"," <td id=\"flight\">Flight No</td>,\n"," <td>Launch site</td>,\n"," <td>Payload mass</td>,\n"," <tr> <td>1</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td><td>300 kg</td></tr>,\n"," <td>1</td>,\n"," <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a></a></td>,\n"," <a></a>,\n"," <td>300 kg</td>,\n"," <tr><td>2</td><td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td><td>94 kg</td></tr>,\n"," <td>2</td>,\n"," <td><a href=\"https://en.wikipedia.org/wiki/Texas\">Texas</a></td>,\n"," <td>94 kg</td>,\n"," <tr><td>3</td><td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td><td>80 kg</td></tr>,\n"," <td>3</td>,\n"," <td><a href=\"https://en.wikipedia.org/wiki/Florida\">Florida</a><a> </a></td>,\n"," <a> </a>,\n"," <td>80 kg</td>]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["table_bs.find_all(href=False)"]},{"cell_type":"markdown","id":"c07eb27e-1548-42d1-9d1a-ed7483d08b03","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```\n","table_bs.find_all(href=False)\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"869dd3e9-7d29-433f-a0f8-005ee08544d0","metadata":{},"source":["Using the soup object <code>soup</code>, find the element with the <code>id</code> attribute content set to <code>\"boldest\"</code>.\n"]},{"cell_type":"code","execution_count":41,"id":"9c4ee6f5-5666-43fd-a1c7-3176e3844756","metadata":{},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["table_bs.find_all(id=\"boldest\")"]},{"cell_type":"markdown","id":"1fffcc74-0e8d-40c1-9292-9f9cbedf0838","metadata":{},"source":["<details><summary>Click here for the solution</summary>\n","\n","```\n","soup.find_all(id=\"boldest\")\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"0c3aa704-ca52-4b44-9bb9-ccaac281ef13","metadata":{},"source":["### string\n"]},{"cell_type":"markdown","id":"e1c9539d-9c2b-431e-ad5f-3de52e0dfb51","metadata":{},"source":["With string you can search for strings instead of tags, where we find all the elments with Florida:\n"]},{"cell_type":"code","execution_count":42,"id":"15b8dc84-e072-42ba-acdf-48784551f80e","metadata":{},"outputs":[{"data":{"text/plain":["['Florida', 'Florida']"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["table_bs.find_all(string=\"Florida\")"]},{"cell_type":"markdown","id":"eb92da7f-8a50-4a09-8ccf-b5009aa834c0","metadata":{},"source":["## find\n"]},{"cell_type":"markdown","id":"711c8beb-cdef-4326-8305-b35f4f8ad5c0","metadata":{},"source":["The <code>find_all()</code> method scans the entire document looking for results. It’s useful if you are looking for one element, as you can use the <code>find()</code> method to find the first element in the document. Consider the following two tables:\n"]},{"cell_type":"code","execution_count":null,"id":"ec8f4a51-d74b-404a-8ce2-3e92d8a678dd","metadata":{},"outputs":[],"source":["%%html\n","<h3>Rocket Launch </h3>\n","\n","<p>\n","<table class='rocket'>\n","  <tr>\n","    <td>Flight No</td>\n","    <td>Launch site</td> \n","    <td>Payload mass</td>\n","  </tr>\n","  <tr>\n","    <td>1</td>\n","    <td>Florida</td>\n","    <td>300 kg</td>\n","  </tr>\n","  <tr>\n","    <td>2</td>\n","    <td>Texas</td>\n","    <td>94 kg</td>\n","  </tr>\n","  <tr>\n","    <td>3</td>\n","    <td>Florida </td>\n","    <td>80 kg</td>\n","  </tr>\n","</table>\n","</p>\n","<p>\n","\n","<h3>Pizza Party  </h3>\n","  \n","    \n","<table class='pizza'>\n","  <tr>\n","    <td>Pizza Place</td>\n","    <td>Orders</td> \n","    <td>Slices </td>\n","   </tr>\n","  <tr>\n","    <td>Domino's Pizza</td>\n","    <td>10</td>\n","    <td>100</td>\n","  </tr>\n","  <tr>\n","    <td>Little Caesars</td>\n","    <td>12</td>\n","    <td >144 </td>\n","  </tr>\n","  <tr>\n","    <td>Papa John's </td>\n","    <td>15 </td>\n","    <td>165</td>\n","  </tr>\n"]},{"cell_type":"markdown","id":"26358b0e-3241-4ea9-a5f0-1f2b94ba3f33","metadata":{},"source":["We store the HTML as a Python string and assign <code>two_tables</code>:\n"]},{"cell_type":"code","execution_count":43,"id":"a4dde735-8119-4414-8826-e9102a4ce7bf","metadata":{},"outputs":[],"source":["two_tables=\"<h3>Rocket Launch </h3><p><table class='rocket'><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table></p><p><h3>Pizza Party  </h3><table class='pizza'><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td >144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr>\""]},{"cell_type":"markdown","id":"c48c1237-3d32-47c5-aac6-b871c4248ea1","metadata":{},"source":["We create a <code>BeautifulSoup</code> object  <code>two_tables_bs</code>\n"]},{"cell_type":"code","execution_count":44,"id":"dce81816-e1ba-43f6-be38-bb2da850519c","metadata":{},"outputs":[],"source":["two_tables_bs= BeautifulSoup(two_tables, 'html.parser')"]},{"cell_type":"markdown","id":"bcfab311-1452-4047-ac00-727c5822f014","metadata":{},"source":["We can find the first table using the tag name table\n"]},{"cell_type":"code","execution_count":45,"id":"5163611b-1c85-4dd2-9755-60a425f49aba","metadata":{},"outputs":[{"data":{"text/plain":["<table class=\"rocket\"><tr><td>Flight No</td><td>Launch site</td> <td>Payload mass</td></tr><tr><td>1</td><td>Florida</td><td>300 kg</td></tr><tr><td>2</td><td>Texas</td><td>94 kg</td></tr><tr><td>3</td><td>Florida </td><td>80 kg</td></tr></table>"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["two_tables_bs.find(\"table\")"]},{"cell_type":"markdown","id":"9d3f2f1b-1c95-448b-8d6d-9983dd674999","metadata":{},"source":["We can filter on the class attribute to find the second table, but because class is a keyword in Python, we add an underscore to differentiate them.\n"]},{"cell_type":"code","execution_count":46,"id":"bd13ff13-15e4-43e2-ab9d-5568712c1dc4","metadata":{},"outputs":[{"data":{"text/plain":["<table class=\"pizza\"><tr><td>Pizza Place</td><td>Orders</td> <td>Slices </td></tr><tr><td>Domino's Pizza</td><td>10</td><td>100</td></tr><tr><td>Little Caesars</td><td>12</td><td>144 </td></tr><tr><td>Papa John's </td><td>15 </td><td>165</td></tr></table>"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["two_tables_bs.find(\"table\",class_='pizza')"]},{"cell_type":"markdown","id":"12927e50-5660-4013-a33b-3778f1003cd8","metadata":{},"source":["<h2 id=\"DSCW\">Downloading And Scraping The Contents Of A Web Page</h2> \n"]},{"cell_type":"markdown","id":"cac77901-1b9a-48f2-a31e-f179554fbc3d","metadata":{},"source":["We Download the contents of the web page:\n"]},{"cell_type":"code","execution_count":47,"id":"adcc5a78-5630-4c8d-99c4-37f33f22cb9e","metadata":{},"outputs":[],"source":["url = \"http://www.ibm.com\""]},{"cell_type":"markdown","id":"463f59ff-96b0-4e20-9922-9938241fefc3","metadata":{},"source":["We use <code>get</code> to download the contents of the webpage in text format and store in a variable called <code>data</code>:\n"]},{"cell_type":"code","execution_count":48,"id":"e6eede6c-20b7-46aa-8b86-1f5aa21f3871","metadata":{},"outputs":[],"source":["data  = requests.get(url).text "]},{"cell_type":"markdown","id":"36e5214f-9b54-458d-96da-116e50a3808f","metadata":{},"source":["We create a <code>BeautifulSoup</code> object using the <code>BeautifulSoup</code> constructor\n"]},{"cell_type":"code","execution_count":49,"id":"1e492a03-f519-4458-8f45-3581d99b93be","metadata":{},"outputs":[],"source":["soup = BeautifulSoup(data,\"html5lib\")  # create a soup object using the variable 'data'"]},{"cell_type":"markdown","id":"37ab294a-a581-427f-a483-140669c9365c","metadata":{},"source":["Scrape all links\n"]},{"cell_type":"code","execution_count":null,"id":"10508361-be47-4853-8501-5364b92a22de","metadata":{},"outputs":[],"source":["for link in soup.find_all('a',href=True):  # in html anchor/link is represented by the tag <a>\n","\n","    print(link.get('href'))\n"]},{"cell_type":"markdown","id":"0fb6a580-9550-46de-8538-f29cdd8d454b","metadata":{},"source":["### Scrape all images Tags\n"]},{"cell_type":"code","execution_count":null,"id":"26b41955-58d5-4440-abe7-390ce40eac8a","metadata":{},"outputs":[],"source":["for link in soup.find_all('img'):# in html image is represented by the tag <img>\n","    print(link)\n","    print(link.get('src'))"]},{"cell_type":"markdown","id":"680fe490-51d8-41e2-9528-c603df60f2e0","metadata":{},"source":["### Scrape data from HTML tables\n"]},{"cell_type":"code","execution_count":51,"id":"f97fde7a-b2e7-4eb1-9652-c13c7e225362","metadata":{},"outputs":[],"source":["#The below url contains an html table with data about colors and color codes.\n","url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/datasets/HTMLColorCodes.html\""]},{"cell_type":"markdown","id":"b3a5a6ae-3c50-4b9b-b548-33bc7ea38c51","metadata":{},"source":["Before proceeding to scrape a web site, you need to examine the contents and the way data is organized on the website. Open the above url in your browser and check how many rows and columns there are in the color table.\n"]},{"cell_type":"code","execution_count":52,"id":"8ce68de6-afda-4b0d-ba77-edf6e39f32c1","metadata":{},"outputs":[],"source":["# get the contents of the webpage in text format and store in a variable called data\n","data  = requests.get(url).text"]},{"cell_type":"code","execution_count":53,"id":"e5870454-bab4-4a8c-ba42-7dcb6cff75c4","metadata":{},"outputs":[],"source":["soup = BeautifulSoup(data,\"html5lib\")"]},{"cell_type":"code","execution_count":54,"id":"44d1446d-da29-4308-b017-0a52f0598a93","metadata":{},"outputs":[{"data":{"text/plain":["<table border=\"1\" class=\"main-table\">\n","   <tbody><tr>\n","      <td>Number </td>\n","      <td>Color</td>\n","      <td>Color Name</td>\n","      <td>Hex Code<br/>#RRGGBB</td>\n","      <td>Decimal Code<br/>(R,G,B)</td>\n","   </tr>\n","   <tr>\n","      <td>1</td>\n","      <td style=\"background:lightsalmon;\"> </td>\n","      <td>lightsalmon</td>\n","      <td>#FFA07A</td>\n","      <td>rgb(255,160,122)</td>\n","   </tr>\n","   <tr>\n","      <td>2</td>\n","      <td style=\"background:salmon;\"> </td>\n","      <td>salmon</td>\n","      <td>#FA8072</td>\n","      <td>rgb(250,128,114)</td>\n","   </tr>\n","   <tr>\n","      <td>3</td>\n","      <td style=\"background:darksalmon;\"> </td>\n","      <td>darksalmon</td>\n","      <td>#E9967A</td>\n","      <td>rgb(233,150,122)</td>\n","   </tr>\n","   <tr>\n","      <td>4</td>\n","      <td style=\"background:lightcoral;\"> </td>\n","      <td>lightcoral</td>\n","      <td>#F08080</td>\n","      <td>rgb(240,128,128)</td>\n","   </tr>\n","   <tr>\n","      <td>5</td>\n","      <td style=\"background:coral;\"> </td>\n","      <td>coral</td>\n","      <td>#FF7F50</td>\n","      <td>rgb(255,127,80)</td>\n","   </tr>\n","   <tr>\n","      <td>6</td>\n","      <td style=\"background:tomato;\"> </td>\n","      <td>tomato</td>\n","      <td>#FF6347</td>\n","      <td>rgb(255,99,71)</td>\n","   </tr>\n","   <tr>\n","      <td>7</td>\n","      <td style=\"background:orangered;\"> </td>\n","      <td>orangered</td>\n","      <td>#FF4500</td>\n","      <td>rgb(255,69,0)</td>\n","   </tr>\n","   <tr>\n","      <td>8</td>\n","      <td style=\"background:gold;\"> </td>\n","      <td>gold</td>\n","      <td>#FFD700</td>\n","      <td>rgb(255,215,0)</td>\n","   </tr>\n","   <tr>\n","      <td>9</td>\n","      <td style=\"background:orange;\"> </td>\n","      <td>orange</td>\n","      <td>#FFA500</td>\n","      <td>rgb(255,165,0)</td>\n","   </tr>\n","   <tr>\n","      <td>10</td>\n","      <td style=\"background:darkorange;\"> </td>\n","      <td>darkorange</td>\n","      <td>#FF8C00</td>\n","      <td>rgb(255,140,0)</td>\n","   </tr>\n","   <tr>\n","      <td>11</td>\n","      <td style=\"background:lightyellow;\"> </td>\n","      <td>lightyellow</td>\n","      <td>#FFFFE0</td>\n","      <td>rgb(255,255,224)</td>\n","   </tr>\n","   <tr>\n","      <td>12</td>\n","      <td style=\"background:lemonchiffon;\"> </td>\n","      <td>lemonchiffon</td>\n","      <td>#FFFACD</td>\n","      <td>rgb(255,250,205)</td>\n","   </tr>\n","   <tr>\n","      <td>13</td>\n","      <td style=\"background:papayawhip;\"> </td>\n","      <td>papayawhip</td>\n","      <td>#FFEFD5</td>\n","      <td>rgb(255,239,213)</td>\n","   </tr>\n","   <tr>\n","      <td>14</td>\n","      <td style=\"background:moccasin;\"> </td>\n","      <td>moccasin</td>\n","      <td>#FFE4B5</td>\n","      <td>rgb(255,228,181)</td>\n","   </tr>\n","   <tr>\n","      <td>15</td>\n","      <td style=\"background:peachpuff;\"> </td>\n","      <td>peachpuff</td>\n","      <td>#FFDAB9</td>\n","      <td>rgb(255,218,185)</td>\n","   </tr>\n","   <tr>\n","      <td>16</td>\n","      <td style=\"background:palegoldenrod;\"> </td>\n","      <td>palegoldenrod</td>\n","      <td>#EEE8AA</td>\n","      <td>rgb(238,232,170)</td>\n","   </tr>\n","   <tr>\n","      <td>17</td>\n","      <td style=\"background:khaki;\"> </td>\n","      <td>khaki</td>\n","      <td>#F0E68C</td>\n","      <td>rgb(240,230,140)</td>\n","   </tr>\n","   <tr>\n","      <td>18</td>\n","      <td style=\"background:darkkhaki;\"> </td>\n","      <td>darkkhaki</td>\n","      <td>#BDB76B</td>\n","      <td>rgb(189,183,107)</td>\n","   </tr>\n","   <tr>\n","      <td>19</td>\n","      <td style=\"background:yellow;\"> </td>\n","      <td>yellow</td>\n","      <td>#FFFF00</td>\n","      <td>rgb(255,255,0)</td>\n","   </tr>\n","   <tr>\n","      <td>20</td>\n","      <td style=\"background:lawngreen;\"> </td>\n","      <td>lawngreen</td>\n","      <td>#7CFC00</td>\n","      <td>rgb(124,252,0)</td>\n","   </tr>\n","   <tr>\n","      <td>21</td>\n","      <td style=\"background:chartreuse;\"> </td>\n","      <td>chartreuse</td>\n","      <td>#7FFF00</td>\n","      <td>rgb(127,255,0)</td>\n","   </tr>\n","   <tr>\n","      <td>22</td>\n","      <td style=\"background:limegreen;\"> </td>\n","      <td>limegreen</td>\n","      <td>#32CD32</td>\n","      <td>rgb(50,205,50)</td>\n","   </tr>\n","   <tr>\n","      <td>23</td>\n","      <td style=\"background:lime;\"> </td>\n","      <td>lime</td>\n","      <td>#00FF00</td>\n","      <td>rgb(0.255.0)</td>\n","   </tr>\n","   <tr>\n","      <td>24</td>\n","      <td style=\"background:forestgreen;\"> </td>\n","      <td>forestgreen</td>\n","      <td>#228B22</td>\n","      <td>rgb(34,139,34)</td>\n","   </tr>\n","   <tr>\n","      <td>25</td>\n","      <td style=\"background:green;\"> </td>\n","      <td>green</td>\n","      <td>#008000</td>\n","      <td>rgb(0,128,0)</td>\n","   </tr>\n","   <tr>\n","      <td>26</td>\n","      <td style=\"background:powderblue;\"> </td>\n","      <td>powderblue</td>\n","      <td>#B0E0E6</td>\n","      <td>rgb(176,224,230)</td>\n","   </tr>\n","   <tr>\n","      <td>27</td>\n","      <td style=\"background:lightblue;\"> </td>\n","      <td>lightblue</td>\n","      <td>#ADD8E6</td>\n","      <td>rgb(173,216,230)</td>\n","   </tr>\n","   <tr>\n","      <td>28</td>\n","      <td style=\"background:lightskyblue;\"> </td>\n","      <td>lightskyblue</td>\n","      <td>#87CEFA</td>\n","      <td>rgb(135,206,250)</td>\n","   </tr>\n","   <tr>\n","      <td>29</td>\n","      <td style=\"background:skyblue;\"> </td>\n","      <td>skyblue</td>\n","      <td>#87CEEB</td>\n","      <td>rgb(135,206,235)</td>\n","   </tr>\n","   <tr>\n","      <td>30</td>\n","      <td style=\"background:deepskyblue;\"> </td>\n","      <td>deepskyblue</td>\n","      <td>#00BFFF</td>\n","      <td>rgb(0,191,255)</td>\n","   </tr>\n","   <tr>\n","      <td>31</td>\n","      <td style=\"background:lightsteelblue;\"> </td>\n","      <td>lightsteelblue</td>\n","      <td>#B0C4DE</td>\n","      <td>rgb(176,196,222)</td>\n","   </tr>\n","   <tr>\n","      <td>32</td>\n","      <td style=\"background:dodgerblue;\"> </td>\n","      <td>dodgerblue</td>\n","      <td>#1E90FF</td>\n","      <td>rgb(30,144,255)</td>\n","   </tr>\n","</tbody></table>"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["#find a html table in the web page\n","table = soup.find('table') # in html table is represented by the tag <table>\n","table"]},{"cell_type":"code","execution_count":55,"id":"480083b6-a5f4-4d1b-b02b-6f77ea4760c7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Color Name--->Hex Code#RRGGBB\n","lightsalmon--->#FFA07A\n","salmon--->#FA8072\n","darksalmon--->#E9967A\n","lightcoral--->#F08080\n","coral--->#FF7F50\n","tomato--->#FF6347\n","orangered--->#FF4500\n","gold--->#FFD700\n","orange--->#FFA500\n","darkorange--->#FF8C00\n","lightyellow--->#FFFFE0\n","lemonchiffon--->#FFFACD\n","papayawhip--->#FFEFD5\n","moccasin--->#FFE4B5\n","peachpuff--->#FFDAB9\n","palegoldenrod--->#EEE8AA\n","khaki--->#F0E68C\n","darkkhaki--->#BDB76B\n","yellow--->#FFFF00\n","lawngreen--->#7CFC00\n","chartreuse--->#7FFF00\n","limegreen--->#32CD32\n","lime--->#00FF00\n","forestgreen--->#228B22\n","green--->#008000\n","powderblue--->#B0E0E6\n","lightblue--->#ADD8E6\n","lightskyblue--->#87CEFA\n","skyblue--->#87CEEB\n","deepskyblue--->#00BFFF\n","lightsteelblue--->#B0C4DE\n","dodgerblue--->#1E90FF\n"]}],"source":["#Get all rows from the table\n","for row in table.find_all('tr'): # in html table row is represented by the tag <tr>\n","    # Get all columns in each row.\n","    cols = row.find_all('td') # in html a column is represented by the tag <td>\n","    color_name = cols[2].string # store the value in column 3 as color_name\n","    color_code = cols[3].text # store the value in column 4 as color_code\n","    print(\"{}--->{}\".format(color_name,color_code))"]},{"cell_type":"markdown","id":"318af9c7-6f36-453e-b912-3e59eab797d7","metadata":{},"source":["## Scraping tables from a Web page using Pandas\n"]},{"cell_type":"markdown","id":"ef0e6c22-5264-4c9c-9fd1-5053c52a80c3","metadata":{},"source":["Particularly for extracting tabular data from a web page, you may also use the `read_html()` method of the Pandas library. \n"]},{"cell_type":"code","execution_count":56,"id":"27812e7d-97ba-4ac2-816d-85073798c90a","metadata":{},"outputs":[],"source":["#The below url contains an html table with data about colors and color codes.\n","url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/datasets/HTMLColorCodes.html\""]},{"cell_type":"markdown","id":"67db0df5-de6f-4ccd-8aae-7d440753632b","metadata":{},"source":["You may extract all the tables from the given webpage simply by using the following commands.\n"]},{"cell_type":"code","execution_count":57,"id":"4478581c-8245-4e76-9607-72b20b38cbf3","metadata":{},"outputs":[{"ename":"ImportError","evalue":"Missing optional dependency 'lxml'.  Use pip or conda to install lxml.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\CxLos\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\compat\\_optional.py:132\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m     module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(name)\n\u001b[0;32m    133\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\CxLos\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n","File \u001b[1;32m<frozen importlib._bootstrap>:1381\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:1354\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:1304\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:1381\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:1354\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n","File \u001b[1;32m<frozen importlib._bootstrap>:1318\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lxml'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32mc:\\Users\\CxLos\\OneDrive\\Desktop\\IBM Data Analyst Professional Certificate\\Python\\4. Python for Data Science\\Week 5 API's & Data Collection\\labs_module 1_Web Scraping_Web-Scraping-Review-Lab.ipynb Cell 137\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/CxLos/OneDrive/Desktop/IBM%20Data%20Analyst%20Professional%20Certificate/Python/4.%20Python%20for%20Data%20Science/Week%205%20API%27s%20%26%20Data%20Collection/labs_module%201_Web%20Scraping_Web-Scraping-Review-Lab.ipynb#Y253sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/CxLos/OneDrive/Desktop/IBM%20Data%20Analyst%20Professional%20Certificate/Python/4.%20Python%20for%20Data%20Science/Week%205%20API%27s%20%26%20Data%20Collection/labs_module%201_Web%20Scraping_Web-Scraping-Review-Lab.ipynb#Y253sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tables \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_html(url)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/CxLos/OneDrive/Desktop/IBM%20Data%20Analyst%20Professional%20Certificate/Python/4.%20Python%20for%20Data%20Science/Week%205%20API%27s%20%26%20Data%20Collection/labs_module%201_Web%20Scraping_Web-Scraping-Review-Lab.ipynb#Y253sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tables\n","File \u001b[1;32mc:\\Users\\CxLos\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\html.py:1245\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[0;32m   1229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(io, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[0;32m   1230\u001b[0m     [\n\u001b[0;32m   1231\u001b[0m         is_file_like(io),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     ]\n\u001b[0;32m   1236\u001b[0m ):\n\u001b[0;32m   1237\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1238\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPassing literal html to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mread_html\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. To read from a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1242\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1243\u001b[0m     )\n\u001b[1;32m-> 1245\u001b[0m \u001b[39mreturn\u001b[39;00m _parse(\n\u001b[0;32m   1246\u001b[0m     flavor\u001b[39m=\u001b[39;49mflavor,\n\u001b[0;32m   1247\u001b[0m     io\u001b[39m=\u001b[39;49mio,\n\u001b[0;32m   1248\u001b[0m     match\u001b[39m=\u001b[39;49mmatch,\n\u001b[0;32m   1249\u001b[0m     header\u001b[39m=\u001b[39;49mheader,\n\u001b[0;32m   1250\u001b[0m     index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m   1251\u001b[0m     skiprows\u001b[39m=\u001b[39;49mskiprows,\n\u001b[0;32m   1252\u001b[0m     parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m   1253\u001b[0m     thousands\u001b[39m=\u001b[39;49mthousands,\n\u001b[0;32m   1254\u001b[0m     attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1255\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1256\u001b[0m     decimal\u001b[39m=\u001b[39;49mdecimal,\n\u001b[0;32m   1257\u001b[0m     converters\u001b[39m=\u001b[39;49mconverters,\n\u001b[0;32m   1258\u001b[0m     na_values\u001b[39m=\u001b[39;49mna_values,\n\u001b[0;32m   1259\u001b[0m     keep_default_na\u001b[39m=\u001b[39;49mkeep_default_na,\n\u001b[0;32m   1260\u001b[0m     displayed_only\u001b[39m=\u001b[39;49mdisplayed_only,\n\u001b[0;32m   1261\u001b[0m     extract_links\u001b[39m=\u001b[39;49mextract_links,\n\u001b[0;32m   1262\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m   1263\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   1264\u001b[0m )\n","File \u001b[1;32mc:\\Users\\CxLos\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\html.py:976\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m retained \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[39mfor\u001b[39;00m flav \u001b[39min\u001b[39;00m flavor:\n\u001b[1;32m--> 976\u001b[0m     parser \u001b[39m=\u001b[39m _parser_dispatch(flav)\n\u001b[0;32m    977\u001b[0m     p \u001b[39m=\u001b[39m parser(\n\u001b[0;32m    978\u001b[0m         io,\n\u001b[0;32m    979\u001b[0m         compiled_match,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    984\u001b[0m         storage_options,\n\u001b[0;32m    985\u001b[0m     )\n\u001b[0;32m    987\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\CxLos\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\html.py:923\u001b[0m, in \u001b[0;36m_parser_dispatch\u001b[1;34m(flavor)\u001b[0m\n\u001b[0;32m    921\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mbs4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    922\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 923\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39;49m\u001b[39mlxml.etree\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    924\u001b[0m \u001b[39mreturn\u001b[39;00m _valid_parsers[flavor]\n","File \u001b[1;32mc:\\Users\\CxLos\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 135\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[0;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n","\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'lxml'.  Use pip or conda to install lxml."]}],"source":["import pandas as pd\n","\n","tables = pd.read_html(url)\n","tables"]},{"cell_type":"markdown","id":"ed8eb3af-38d9-47f9-8dea-da02c65f453d","metadata":{},"source":["`tables` is now a list of dataframes representing the tables from the web page, in the sequence of their appearance. In the current  URL, there is only a single table, so the same can be accessed as shown below.\n"]},{"cell_type":"code","execution_count":null,"id":"2e1e6f4d-dc23-4b9f-9012-d78843d6e76a","metadata":{},"outputs":[],"source":["tables[0]"]},{"cell_type":"markdown","id":"f3fea0e9-4074-4900-9dc6-48932e4325a9","metadata":{},"source":["## Authors\n"]},{"cell_type":"markdown","id":"f831324c-8d79-4885-a6c2-133614977b82","metadata":{},"source":["Ramesh Sannareddy\n"]},{"cell_type":"markdown","id":"67adaef3-64fd-47c7-a83d-4959943df6da","metadata":{},"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"92e66d43-a71c-436f-8be6-ffc7e069047d","metadata":{},"source":["Rav Ahuja\n","\n","Abhishek Gagneja\n"]},{"cell_type":"markdown","id":"91f228e5-a6b5-4b3e-8990-1b2f9b6777b8","metadata":{},"source":["## Change Log\n"]},{"cell_type":"markdown","id":"88cb197f-504d-4291-996a-39d48234b53a","metadata":{},"source":["| Date (YYYY-MM-DD) | Version | Changed By            |          Change Description         |\n","| ----------------- | ------- | ----------------------| ----------------------------------- |\n","| 2023-11-02 | 1.1 | Abhishek Gagneja | Updated instructions, added web scraping using Pandas |\n","| 2023-06-11        | 1.0     | Akansha Yadav         |   Spell check                       |\n","| 2020-10-17        | 0.1     | Joseph Santarcangelo  |  Created initial version of the lab |\n"]},{"cell_type":"markdown","id":"3c856391-6573-437d-a270-953eb01cd1da","metadata":{},"source":["Copyright © 2023 IBM Corporation. This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0101ENSkillsNetwork19487395-2021-01-01).\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":4}
